{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b97685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import base64\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import ssl\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onedrive_directdownload (onedrive_link):\n",
    "    data_bytes64 = base64.b64encode(bytes(onedrive_link, 'utf-8'))\n",
    "    data_bytes64_String = data_bytes64.decode('utf-8').replace('/','_').replace('+','-').rstrip(\"=\")\n",
    "    resultUrl = f\"https://api.onedrive.com/v1.0/shares/u!{data_bytes64_String}/root/content\"\n",
    "    return resultUrl\n",
    "\n",
    "onedrive_link=create_onedrive_directdownload(\"https://1drv.ms/x/s!AoiE7xOoBAsngsd-PpntYUKRt9nOQg?e=YO5X5h\")\n",
    "print(onedrive_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(onedrive_link)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99beecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df['content'].isna()]\n",
    "df=df[df['labeled']==True]\n",
    "col=['message_id','subject','content','cat_1_level_2']\n",
    "df=df.loc[:,col]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content']=df['content'].replace({'\\n':' ','\\t':' '},regex=True)\n",
    "df['content']=df['content'].replace({',':'',},regex=True)\n",
    "df['content']=df['content'].replace({';':'',},regex=True)\n",
    "\n",
    "df['content']=df['content'].replace({'\"':'',},regex=True)\n",
    "df['subject']=df['subject'].replace({'\"':''},regex=True)\n",
    "\n",
    "df['content']=df['content'].replace({\"'\":\"\"},regex=True)\n",
    "df['subject']=df['subject'].replace({\"'\":\"\"},regex=True)\n",
    "\n",
    "df=df.loc[(~df['subject'].str.startswith('FW:',na=False))  ,:]\n",
    "df.loc[(~df['subject'].str.startswith('FW:',na=False))  ,:]\n",
    "\n",
    "df.loc[df['content'].str.contains('-----Original Message-----'),'content']=df.loc[df['content'].str.contains('-----Original Message-----'),'content'].str.extract(r'([\\s\\S]*?)'+r'-----Original Message-----',expand=False)\n",
    "df.loc[df['content'].str.contains('----- Forwarded by'),'content']=df.loc[df['content'].str.contains('----- Forwarded by'),'content'].str.extract(r'([\\s\\S]*?)'+r'----- Forwarded by',expand=False)\n",
    "df.loc[df['content'].str.contains('---------------------- Forwarded by'),'content']=df.loc[df['content'].str.contains('---------------------- Forwarded by'),'content'].str.extract(r'([\\s\\S]*?)'+r'---------------------- Forwarded by',expand=False)\n",
    "df.loc[df['content'].str.contains('-------------------------- Sent from my BlackBerry'),'content']=df.loc[df['content'].str.contains('-------------------------- Sent from my BlackBerry'),'content'].str.extract(r'([\\s\\S]*?)'+r'-------------------------- Sent from my BlackBerry',expand=False)\n",
    "\n",
    "# remove all the numbers\n",
    "df['content']=df['content'].replace({r'\\d+':''},regex=True)\n",
    "\n",
    "# remove email ids\n",
    "df['content']=df['content'].replace({r'\\S*@\\S*\\s?':''},regex=True)\n",
    "\n",
    "'''\n",
    "# remove everything after To: From: Re: Subject: etc..\n",
    "df['content']=df['content'].replace({r'To:.+':'',r'From:.+':'',r'Re:.+':'',r'Subject:.+':'',r'Date:.+':''},regex=True)\n",
    "'''\n",
    "\n",
    "# remove just To: From: Re: Subject: etc.. texts\n",
    "df['content']=df['content'].replace({r'To:':'',r'From:':'',r'Re:':'',r'Subject:':'',r'Date:':''},regex=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df.to_csv('enron_removed.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message_id']=df['message_id'].astype(str)\n",
    "df['subject']=df['subject'].astype(str)\n",
    "df['content']=df['content'].astype(str)\n",
    "\n",
    "# to lower type\n",
    "df['content']=df['content'].str.lower()\n",
    "df['subject']=df['subject'].str.lower()\n",
    "\n",
    "# remove punctuations\n",
    "df['content'] = df['content'].str.replace('[^\\w\\s]','')\n",
    "df['subject'] = df['subject'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "'''\n",
    "# remove combination of words and digits\n",
    "df['content'] = df['content'].str.replace('W*dw*','')\n",
    "df['subject'] = df['subject'].str.replace('W*dw*','')\n",
    "'''\n",
    "\n",
    "# tokenization\n",
    "df[\"content\"] = df[\"content\"].apply(lambda x: nltk.word_tokenize(x))\n",
    "df[\"subject\"] = df[\"subject\"].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "'''\n",
    "# porter stemming\n",
    "ps = PorterStemmer()\n",
    "df[\"content\"] = df[\"content\"].apply(lambda x: [ps.stem(i) for i in x])\n",
    "df[\"subject\"] = df[\"subject\"].apply(lambda x: [ps.stem(i) for i in x])\n",
    "'''\n",
    "'''\n",
    "\n",
    "# snowball stemming\n",
    "ss = SnowballStemmer(language = \"english\")\n",
    "df[\"content\"] = df[\"content\"].apply(lambda x: [ss.stem(i) for i in x])\n",
    "df[\"subject\"] = df[\"subject\"].apply(lambda x: [ss.stem(i) for i in x])\n",
    "'''\n",
    "\n",
    "# lemmantization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df[\"content\"] = df[\"content\"].apply(lambda x: [lemmatizer.lemmatize(i) for i in x])\n",
    "df[\"subject\"] = df[\"subject\"].apply(lambda x: [lemmatizer.lemmatize(i) for i in x])\n",
    "\n",
    "# join text back\n",
    "def join_words(text):\n",
    "    return \" \".join(text)\n",
    "df[\"subject\"] = df[\"subject\"].apply(lambda x: join_words(x))\n",
    "df[\"content\"] = df[\"content\"].apply(lambda x: join_words(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c816fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content=df['content']\n",
    "df_message_id=df['message_id']\n",
    "df_subject=df['subject']\n",
    "df_label=df['cat_1_level_2'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e51a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('BERT_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b898035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# character length\n",
    "df['content'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total word length\n",
    "def sentence_len(sentence):\n",
    "    return len(sentence.split())\n",
    "\n",
    "df['content'].apply(lambda x: sentence_len(x)).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65364507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word length\n",
    "df['content'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ea4db",
   "metadata": {},
   "source": [
    "1.\tCC\tCoordinating conjunction\n",
    "2.\tCD\tCardinal number\n",
    "3.\tDT\tDeterminer\n",
    "4.\tEX\tExistential there\n",
    "5.\tFW\tForeign word\n",
    "6.\tIN\tPreposition or subordinating conjunction\n",
    "7.\tJJ\tAdjective\n",
    "8.\tJJR\tAdjective, comparative\n",
    "9.\tJJS\tAdjective, superlative\n",
    "10.\tLS\tList item marker\n",
    "11.\tMD\tModal\n",
    "12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural\n",
    "16.\tPDT\tPredeterminer\n",
    "17.\tPOS\tPossessive ending\n",
    "18.\tPRP\tPersonal pronoun\n",
    "19.\tPRP$\tPossessive pronoun\n",
    "20.\tRB\tAdverb\n",
    "21.\tRBR\tAdverb, comparative\n",
    "22.\tRBS\tAdverb, superlative\n",
    "23.\tRP\tParticle\n",
    "24.\tSYM\tSymbol\n",
    "25.\tTO\tto\n",
    "26.\tUH\tInterjection\n",
    "27.\tVB\tVerb, base form\n",
    "28.\tVBD\tVerb, past tense\n",
    "29.\tVBG\tVerb, gerund or present participle\n",
    "30.\tVBN\tVerb, past participle\n",
    "31.\tVBP\tVerb, non-3rd person singular present\n",
    "32.\tVBZ\tVerb, 3rd person singular present\n",
    "33.\tWDT\tWh-determiner\n",
    "34.\tWP\tWh-pronoun\n",
    "35.\tWP$$\tPossessive wh-pronoun\n",
    "36.\tWRB\tWh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128cb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count\n",
    "new= df['content'].str.split()\n",
    "new=df['content'].values.tolist()\n",
    "corpus=[word for i in new for word in i.split()]\n",
    "wo_li={}\n",
    "for wo in corpus: \n",
    "    wo_li[wo] = wo_li.get(wo, 0) + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbda3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_li = dict(sorted(wo_li.items(), key=lambda item: item[1], reverse=True))\n",
    "wo_li\n",
    "\n",
    "pos_tags = pos_tag(wo_li.keys())\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66303cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_li={}\n",
    "for x,pos in pos_tags: \n",
    "    pos_li[pos] = pos_li.get(pos, 0) + 1 \n",
    "pos_li = dict(sorted(pos_li.items(), key=lambda item: item[1], reverse=True))\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(pos_li.keys())[:15],y=list(pos_li.values())[:15],palette='viridis')\n",
    "plt.title('POS Distribution',size=22)\n",
    "plt.xlabel('Part of Speech')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f829d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b0075",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4fcc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stop words\n",
    "stop_wo_li={}\n",
    "for wo,i in wo_li.items():\n",
    "    if wo in stop:\n",
    "        stop_wo_li[wo]=i\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(stop_wo_li.keys())[:15],y=list(stop_wo_li.values())[:15],palette='viridis')\n",
    "plt.title('Stop Words Distribution',size=22)\n",
    "plt.xlabel('Stop Word')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546aee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not stop words\n",
    "for st_wo in stop:\n",
    "    if st_wo in wo_li:\n",
    "        wo_li.pop(st_wo)\n",
    "wo_li\n",
    "plt.bar(list(wo_li.keys())[:5],list(wo_li.values())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645dca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# filtering out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stop_words(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "df['content'] = df['content'].apply(remove_stop_words)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b1e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a74793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# for removing names\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp('jim authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "print([(X.text, X.label_) for X in doc.ents])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('BERT_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429cbad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu1",
   "language": "python",
   "name": "gpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
